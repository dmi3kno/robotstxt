% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/get_robotstxt.R
\name{get_robotstxt}
\alias{get_robotstxt}
\title{downloading robots.txt file}
\usage{
get_robotstxt(domain, warn = TRUE, force = FALSE,
  user_agent = utils::sessionInfo()$R.version$version.string,
  ssl_verifypeer = c(1, 0), encoding = "UTF-8",
  rt_request_handler = robotstxt::rt_request_handler,
  rt_robotstxt_http_getter = robotstxt::get_robotstxt_http_get,
  rt_event_handler = robotstxt::default_event_handler)
}
\arguments{
\item{domain}{domain from which to download robots.txt file}

\item{warn}{warn about being unable to download domain/robots.txt because of}

\item{force}{if TRUE instead of using possible cached results the function
will re-download the robotstxt file HTTP response status 404. If this
happens,}

\item{user_agent}{HTTP user-agent string to be used to retrieve robots.txt
file from domain}

\item{ssl_verifypeer}{analog to CURL option
\url{https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html} -- and
might help with robots.txt file retrieval in some cases}

\item{encoding}{Encoding of the robots.txt file.}

\item{rt_request_handler}{handler function that handles request according to
the event handlers specified}

\item{rt_robotstxt_http_getter}{function that executes HTTP request}

\item{rt_event_handler}{a list of lists determining the interpretation of robotstxts in case of specific evetns and triggers, see \link{default_event_handler}}
}
\description{
downloading robots.txt file
}
